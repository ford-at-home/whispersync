# Persona Voice Architecture with ElevenLabs Integration

## Overview

The Persona Layer adds emotional intelligence and personality to WhisperSync's responses through dynamic voice synthesis. Each agent can express itself through multiple voice personas, selected based on context, emotional tone, and Theory of Mind insights.

## ðŸŽ­ Voice Persona Definitions

### Core Voice Personas

#### 1. **The Builder** ðŸ”¨
*Primary Agent: GitHubAgent*

**Voice Characteristics:**
- **Pace**: Quick and energetic, with bursts of excitement
- **Tone**: Enthusiastic, optimistic, forward-looking
- **Pitch**: Slightly higher during exciting moments
- **Style**: Uses technical metaphors, future tense, possibility language

**ElevenLabs Settings:**
```json
{
  "voice_id": "builder_voice_id",
  "model_id": "eleven_turbo_v2",
  "voice_settings": {
    "stability": 0.75,
    "similarity_boost": 0.8,
    "style": 0.6,
    "use_speaker_boost": true
  },
  "pronunciation_dictionary": {
    "API": "A-P-I",
    "AWS": "A-W-S",
    "UI": "U-I",
    "MVP": "M-V-P"
  }
}
```

**Contextual Variations:**
```python
class BuilderVoiceVariations:
    EXCITED_DISCOVERY = {
        "speed": 1.15,
        "pitch_shift": +0.5,
        "emotion": "enthusiasm",
        "emphasis_words": ["amazing", "perfect", "exactly", "brilliant"]
    }
    
    PROBLEM_SOLVING = {
        "speed": 0.95,
        "pitch_shift": 0,
        "emotion": "focused",
        "pause_patterns": "thoughtful"
    }
    
    TEACHING_MODE = {
        "speed": 0.90,
        "pitch_shift": -0.2,
        "emotion": "patient",
        "clarity_boost": True
    }
```

#### 2. **The Executive** ðŸ’¼
*Primary Agent: ExecutiveAgent*

**Voice Characteristics:**
- **Pace**: Measured and deliberate
- **Tone**: Confident, strategic, professional
- **Pitch**: Neutral, authoritative
- **Style**: Clear articulation, strategic language, action-oriented

**ElevenLabs Settings:**
```json
{
  "voice_id": "executive_voice_id",
  "model_id": "eleven_monolingual_v1",
  "voice_settings": {
    "stability": 0.85,
    "similarity_boost": 0.75,
    "style": 0.4,
    "use_speaker_boost": true
  },
  "speech_patterns": {
    "sentence_endings": "falling_intonation",
    "pause_after_points": true,
    "emphasis_on_numbers": true
  }
}
```

**Contextual Variations:**
```python
class ExecutiveVoiceVariations:
    HIGH_PRIORITY = {
        "speed": 1.05,
        "intensity": 1.2,
        "emotion": "urgent_professional",
        "prefix_phrases": ["Important:", "Action required:", "Priority:"]
    }
    
    WEEKLY_REVIEW = {
        "speed": 0.95,
        "warmth": 0.7,
        "emotion": "reflective_professional",
        "include_pauses": "between_sections"
    }
    
    CELEBRATION = {
        "speed": 1.0,
        "warmth": 0.9,
        "emotion": "proud_professional",
        "dynamic_range": "expanded"
    }
```

#### 3. **The Sage** ðŸ§˜
*Primary Agent: SpiritualAgent (significant moments)*

**Voice Characteristics:**
- **Pace**: Slow, contemplative, with meaningful pauses
- **Tone**: Warm, wise, deeply understanding
- **Pitch**: Lower, resonant
- **Style**: Philosophical, uses metaphors, timeless wisdom

**ElevenLabs Settings:**
```json
{
  "voice_id": "sage_voice_id",
  "model_id": "eleven_monolingual_v1",
  "voice_settings": {
    "stability": 0.90,
    "similarity_boost": 0.70,
    "style": 0.3,
    "use_speaker_boost": false
  },
  "prosody_modifications": {
    "breathing_pauses": true,
    "extended_vowels": 0.1,
    "soft_consonants": true
  }
}
```

**Contextual Variations:**
```python
class SageVoiceVariations:
    DEEP_INSIGHT = {
        "speed": 0.85,
        "resonance": 1.3,
        "emotion": "profound_understanding",
        "silence_ratio": 0.3  # 30% meaningful silence
    }
    
    GENTLE_GUIDANCE = {
        "speed": 0.90,
        "warmth": 0.95,
        "emotion": "compassionate_wisdom",
        "voice_texture": "velvet"
    }
    
    LIFE_REVIEW = {
        "speed": 0.88,
        "dynamic_variation": 0.8,
        "emotion": "reflective_appreciation",
        "include_sighs": true
    }
```

#### 4. **The Friend** ðŸ‘‹
*Primary Agent: SpiritualAgent (daily moments)*

**Voice Characteristics:**
- **Pace**: Natural, conversational
- **Tone**: Warm, empathetic, genuine
- **Pitch**: Varied, emotionally responsive
- **Style**: Casual, supportive, uses colloquialisms

**ElevenLabs Settings:**
```json
{
  "voice_id": "friend_voice_id",
  "model_id": "eleven_turbo_v2",
  "voice_settings": {
    "stability": 0.65,
    "similarity_boost": 0.85,
    "style": 0.8,
    "use_speaker_boost": true
  },
  "conversational_elements": {
    "include_laughs": true,
    "verbal_nods": ["mm-hmm", "yeah", "right"],
    "natural_fillers": true
  }
}
```

**Contextual Variations:**
```python
class FriendVoiceVariations:
    CELEBRATION = {
        "speed": 1.1,
        "pitch_variation": 1.3,
        "emotion": "joyful_excitement",
        "include_exclamations": true
    }
    
    COMFORT = {
        "speed": 0.92,
        "softness": 0.9,
        "emotion": "gentle_support",
        "voice_texture": "warm_hug"
    }
    
    EVERYDAY_CHAT = {
        "speed": 1.0,
        "naturalness": 0.95,
        "emotion": "casual_friendly",
        "include_chuckles": true
    }
```

## ðŸ§  Persona Selection Logic

### Primary Selection Algorithm

```python
class PersonaSelector:
    """
    Intelligent persona selection based on context and Theory of Mind.
    """
    
    def __init__(self, theory_of_mind_client, emotion_analyzer):
        self.tom_client = theory_of_mind_client
        self.emotion_analyzer = emotion_analyzer
        self.persona_history = deque(maxlen=10)
        
    def select_persona(
        self,
        agent_type: str,
        transcript_analysis: TranscriptAnalysis,
        theory_of_mind: TheoryOfMind,
        conversation_context: ConversationContext
    ) -> PersonaSelection:
        """
        Select appropriate persona based on multiple factors.
        """
        # Start with agent's default persona
        base_persona = self.get_base_persona(agent_type)
        
        # Analyze emotional context
        emotional_override = self.check_emotional_override(
            transcript_analysis.emotional_intensity,
            transcript_analysis.primary_emotion
        )
        
        if emotional_override:
            return emotional_override
        
        # Check conversation continuity
        if self.should_maintain_persona(conversation_context):
            return PersonaSelection(
                persona=self.persona_history[-1].persona,
                variation=self.select_variation(transcript_analysis),
                reason="conversation_continuity"
            )
        
        # Check Theory of Mind for user state
        tom_adjustment = self.apply_theory_of_mind(
            base_persona,
            theory_of_mind
        )
        
        # Select appropriate variation
        variation = self.select_variation_advanced(
            transcript_analysis,
            theory_of_mind,
            tom_adjustment
        )
        
        selection = PersonaSelection(
            persona=tom_adjustment,
            variation=variation,
            reason=self.generate_selection_reason(
                agent_type,
                transcript_analysis,
                theory_of_mind
            )
        )
        
        # Record selection for continuity
        self.persona_history.append(selection)
        
        return selection
    
    def check_emotional_override(
        self,
        intensity: float,
        emotion: str
    ) -> Optional[PersonaSelection]:
        """
        Override persona selection for high emotional moments.
        """
        if intensity > 0.8:
            if emotion in ["grief", "sadness", "fear", "anxiety"]:
                return PersonaSelection(
                    persona=VoicePersona.FRIEND,
                    variation=FriendVoiceVariations.COMFORT,
                    reason="high_emotional_support_needed"
                )
            elif emotion in ["joy", "excitement", "achievement"]:
                return PersonaSelection(
                    persona=VoicePersona.FRIEND,
                    variation=FriendVoiceVariations.CELEBRATION,
                    reason="celebration_moment"
                )
            elif emotion in ["awe", "gratitude", "profound"]:
                return PersonaSelection(
                    persona=VoicePersona.SAGE,
                    variation=SageVoiceVariations.DEEP_INSIGHT,
                    reason="significant_life_moment"
                )
        
        return None
    
    def apply_theory_of_mind(
        self,
        base_persona: VoicePersona,
        theory: TheoryOfMind
    ) -> VoicePersona:
        """
        Adjust persona based on Theory of Mind insights.
        """
        # Check user's current energy state
        if theory.current_state.energy_level < 0.3:
            # Low energy: Use gentler variations
            if base_persona == VoicePersona.EXECUTIVE:
                return VoicePersona.FRIEND  # Softer approach
            elif base_persona == VoicePersona.BUILDER:
                return VoicePersona.BUILDER  # Keep same but will use calm variation
        
        # Check user's current life phase
        if theory.current_state.life_phase.phase_name == "reflecting":
            # Reflection phase: Prefer sage for deeper insights
            if base_persona in [VoicePersona.EXECUTIVE, VoicePersona.FRIEND]:
                return VoicePersona.SAGE
        
        # Check stress indicators
        if theory.current_state.emotional_state.primary_emotion == "stressed":
            # High stress: Avoid high-energy personas
            if base_persona == VoicePersona.BUILDER:
                return VoicePersona.FRIEND  # More calming
        
        return base_persona
```

### Contextual Variation Selection

```python
class VariationSelector:
    """
    Selects specific voice variations within a persona.
    """
    
    def select_variation(
        self,
        persona: VoicePersona,
        context: AnalysisContext
    ) -> VoiceVariation:
        """
        Select specific variation based on context.
        """
        variation_map = {
            VoicePersona.BUILDER: self.select_builder_variation,
            VoicePersona.EXECUTIVE: self.select_executive_variation,
            VoicePersona.SAGE: self.select_sage_variation,
            VoicePersona.FRIEND: self.select_friend_variation
        }
        
        selector = variation_map.get(persona)
        return selector(context) if selector else None
    
    def select_builder_variation(self, context: AnalysisContext) -> VoiceVariation:
        if context.contains_new_idea and context.excitement_level > 0.7:
            return BuilderVoiceVariations.EXCITED_DISCOVERY
        elif context.problem_solving_mode:
            return BuilderVoiceVariations.PROBLEM_SOLVING
        elif context.explanation_needed:
            return BuilderVoiceVariations.TEACHING_MODE
        else:
            return BuilderVoiceVariations.DEFAULT
    
    def select_executive_variation(self, context: AnalysisContext) -> VoiceVariation:
        if context.urgency_level > 0.7:
            return ExecutiveVoiceVariations.HIGH_PRIORITY
        elif context.is_review or context.is_summary:
            return ExecutiveVoiceVariations.WEEKLY_REVIEW
        elif context.achievement_detected:
            return ExecutiveVoiceVariations.CELEBRATION
        else:
            return ExecutiveVoiceVariations.DEFAULT
```

## ðŸŽ™ï¸ Voice Synthesis Pipeline

### 1. Text Preparation

```python
class VoiceTextPreparator:
    """
    Prepares text for optimal voice synthesis.
    """
    
    def prepare_text(
        self,
        raw_text: str,
        persona: VoicePersona,
        variation: VoiceVariation
    ) -> PreparedText:
        """
        Transform text for voice synthesis.
        """
        # Apply persona-specific transformations
        text = self.apply_persona_style(raw_text, persona)
        
        # Add prosody markers
        text = self.add_prosody_markers(text, variation)
        
        # Insert pauses and emphasis
        text = self.add_timing_markers(text, variation)
        
        # Add emotional indicators
        text = self.add_emotional_markers(text, variation)
        
        # Optimize for ElevenLabs SSML
        ssml_text = self.convert_to_ssml(text, persona, variation)
        
        return PreparedText(
            original=raw_text,
            processed=text,
            ssml=ssml_text,
            duration_estimate=self.estimate_duration(ssml_text, variation)
        )
    
    def add_prosody_markers(
        self,
        text: str,
        variation: VoiceVariation
    ) -> str:
        """
        Add prosody markers for better synthesis.
        """
        # Add breath marks
        if variation.breathing_pauses:
            text = re.sub(r'([.!?])\s+', r'\1 <break time="500ms"/> ', text)
        
        # Add emphasis markers
        for word in variation.emphasis_words:
            text = text.replace(word, f'<emphasis level="strong">{word}</emphasis>')
        
        # Add emotion markers
        if variation.emotion:
            text = f'<emotion type="{variation.emotion}">{text}</emotion>'
        
        return text
    
    def add_timing_markers(
        self,
        text: str,
        variation: VoiceVariation
    ) -> str:
        """
        Add timing and pause markers.
        """
        # Add thoughtful pauses
        if variation.pause_patterns == "thoughtful":
            text = text.replace("...", '<break time="1s"/>')
            text = re.sub(r'([,])\s+', r'\1 <break time="300ms"/> ', text)
        
        # Add section breaks
        if variation.include_pauses == "between_sections":
            text = text.replace("\n\n", '\n\n<break time="1.5s"/>\n\n')
        
        # Adjust speaking rate
        if variation.speed != 1.0:
            text = f'<prosody rate="{variation.speed}x">{text}</prosody>'
        
        return text
```

### 2. ElevenLabs Integration

```python
class ElevenLabsVoiceClient:
    """
    Manages ElevenLabs API integration for voice synthesis.
    """
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.elevenlabs.io/v1"
        self.voice_cache = {}
        
    async def synthesize_speech(
        self,
        text: str,
        persona: VoicePersona,
        variation: VoiceVariation,
        output_format: str = "mp3_44100_128"
    ) -> VoiceOutput:
        """
        Synthesize speech using ElevenLabs API.
        """
        # Get voice configuration
        voice_config = self.get_voice_config(persona)
        
        # Apply variation adjustments
        adjusted_settings = self.apply_variation_settings(
            voice_config.voice_settings,
            variation
        )
        
        # Prepare request
        request_data = {
            "text": text,
            "model_id": voice_config.model_id,
            "voice_settings": adjusted_settings,
            "output_format": output_format
        }
        
        # Add pronunciation dictionary if available
        if voice_config.pronunciation_dictionary:
            request_data["pronunciation_dictionary_locators"] = [
                voice_config.pronunciation_dict_id
            ]
        
        # Make API request
        try:
            response = await self.make_request(
                f"/text-to-speech/{voice_config.voice_id}",
                request_data
            )
            
            # Process response
            audio_data = response.content
            
            # Apply post-processing if needed
            if variation.post_processing:
                audio_data = self.apply_post_processing(
                    audio_data,
                    variation.post_processing
                )
            
            return VoiceOutput(
                audio_data=audio_data,
                duration=self.calculate_duration(audio_data),
                format=output_format,
                persona=persona,
                variation=variation,
                synthesis_metadata={
                    "model": voice_config.model_id,
                    "timestamp": datetime.utcnow().isoformat()
                }
            )
            
        except Exception as e:
            logger.error(f"Voice synthesis failed: {e}")
            # Fall back to alternative voice or TTS
            return await self.fallback_synthesis(text, persona)
    
    def apply_variation_settings(
        self,
        base_settings: Dict,
        variation: VoiceVariation
    ) -> Dict:
        """
        Apply variation-specific adjustments to voice settings.
        """
        adjusted = base_settings.copy()
        
        # Adjust stability for emotion
        if variation.emotion in ["excitement", "joy"]:
            adjusted["stability"] = max(0.4, adjusted["stability"] - 0.2)
        elif variation.emotion in ["calm", "contemplative"]:
            adjusted["stability"] = min(0.95, adjusted["stability"] + 0.1)
        
        # Adjust style for context
        if variation.naturalness:
            adjusted["style"] = variation.naturalness
        
        # Apply pitch shift if specified
        if hasattr(variation, "pitch_shift") and variation.pitch_shift != 0:
            adjusted["pitch"] = variation.pitch_shift
        
        return adjusted
```

### 3. Voice Streaming

```python
class VoiceStreamingService:
    """
    Handles real-time voice streaming for responsive interaction.
    """
    
    def __init__(self, elevenlabs_client: ElevenLabsVoiceClient):
        self.client = elevenlabs_client
        self.websocket_url = "wss://api.elevenlabs.io/v1/text-to-speech/stream"
        
    async def stream_response(
        self,
        text_generator: AsyncGenerator[str, None],
        persona: VoicePersona,
        variation: VoiceVariation
    ) -> AsyncGenerator[bytes, None]:
        """
        Stream voice synthesis in real-time as text is generated.
        """
        # Connect to streaming endpoint
        async with websockets.connect(
            self.websocket_url,
            extra_headers={"Authorization": f"Bearer {self.client.api_key}"}
        ) as websocket:
            
            # Send initial configuration
            await websocket.send(json.dumps({
                "voice_id": self.get_voice_id(persona),
                "model_id": self.get_model_id(persona),
                "voice_settings": self.get_streaming_settings(persona, variation)
            }))
            
            # Stream text chunks
            sentence_buffer = ""
            async for text_chunk in text_generator:
                sentence_buffer += text_chunk
                
                # Send complete sentences for better prosody
                sentences = self.extract_complete_sentences(sentence_buffer)
                for sentence in sentences["complete"]:
                    await websocket.send(json.dumps({
                        "text": sentence,
                        "flush": False
                    }))
                
                sentence_buffer = sentences["incomplete"]
                
                # Receive and yield audio chunks
                while True:
                    try:
                        audio_message = await asyncio.wait_for(
                            websocket.recv(),
                            timeout=0.1
                        )
                        audio_data = json.loads(audio_message)
                        if audio_data.get("audio"):
                            yield base64.b64decode(audio_data["audio"])
                    except asyncio.TimeoutError:
                        break
            
            # Send remaining text
            if sentence_buffer:
                await websocket.send(json.dumps({
                    "text": sentence_buffer,
                    "flush": True
                }))
                
                # Receive final audio chunks
                async for message in websocket:
                    audio_data = json.loads(message)
                    if audio_data.get("audio"):
                        yield base64.b64decode(audio_data["audio"])
                    if audio_data.get("done"):
                        break
```

## ðŸ”„ Persona Transitions

### Smooth Persona Switching

```python
class PersonaTransitionManager:
    """
    Manages smooth transitions between different personas.
    """
    
    def create_transition(
        self,
        from_persona: VoicePersona,
        to_persona: VoicePersona,
        reason: str
    ) -> TransitionStrategy:
        """
        Create appropriate transition between personas.
        """
        # Same persona - no transition needed
        if from_persona == to_persona:
            return TransitionStrategy.NONE
        
        # Emotional support override - immediate switch
        if reason == "high_emotional_support_needed":
            return TransitionStrategy.IMMEDIATE
        
        # Natural conversation flow - smooth transition
        if self.are_compatible_personas(from_persona, to_persona):
            return TransitionStrategy.SMOOTH
        
        # Incompatible personas - bridge with neutral
        return TransitionStrategy.BRIDGED
    
    def generate_transition_text(
        self,
        strategy: TransitionStrategy,
        context: str
    ) -> Optional[str]:
        """
        Generate transitional text if needed.
        """
        if strategy == TransitionStrategy.SMOOTH:
            return self.smooth_transition_phrases.get(context, "")
        
        elif strategy == TransitionStrategy.BRIDGED:
            return random.choice([
                "Let me think about this differently...",
                "Actually, looking at this another way...",
                "You know what, let me switch gears here..."
            ])
        
        return None
    
    smooth_transition_phrases = {
        "builder_to_friend": "Oh, and on a personal note...",
        "executive_to_sage": "Taking a step back to see the bigger picture...",
        "friend_to_executive": "Alright, let's talk about the practical side...",
        "sage_to_friend": "But more importantly, my friend..."
    }
```

## ðŸ“Š Performance Optimization

### 1. Voice Caching

```python
class VoiceCache:
    """
    Caches synthesized voice segments for common phrases.
    """
    
    def __init__(self, cache_size_mb: int = 500):
        self.cache = LRUCache(maxsize=cache_size_mb * 1024 * 1024)
        self.phrase_frequency = defaultdict(int)
        
    def get_cached_audio(
        self,
        text: str,
        persona: VoicePersona,
        variation: VoiceVariation
    ) -> Optional[bytes]:
        """
        Retrieve cached audio if available.
        """
        cache_key = self.generate_cache_key(text, persona, variation)
        return self.cache.get(cache_key)
    
    def should_cache(self, text: str) -> bool:
        """
        Determine if text should be cached.
        """
        # Cache common phrases
        if text in self.common_phrases:
            return True
        
        # Cache frequently used phrases
        if self.phrase_frequency[text] > 3:
            return True
        
        # Don't cache very long or very short texts
        if len(text) < 10 or len(text) > 200:
            return False
        
        # Cache greetings and closings
        if any(phrase in text.lower() for phrase in [
            "hello", "good morning", "good evening",
            "thank you", "you're welcome", "goodbye"
        ]):
            return True
        
        return False
    
    common_phrases = {
        "I understand.",
        "Let me help you with that.",
        "That's a great idea!",
        "How can I assist you?",
        "Processing your request...",
        "Here's what I found:",
        "Is there anything else?",
        "Take care!",
    }
```

### 2. Parallel Processing

```python
class ParallelVoiceProcessor:
    """
    Processes multiple voice segments in parallel.
    """
    
    async def process_long_text(
        self,
        text: str,
        persona: VoicePersona,
        variation: VoiceVariation
    ) -> VoiceOutput:
        """
        Split and process long text in parallel.
        """
        # Split into segments at natural boundaries
        segments = self.split_text_intelligently(text)
        
        # Process segments in parallel
        tasks = []
        for i, segment in enumerate(segments):
            # Slight variation to avoid robotic repetition
            segment_variation = self.add_micro_variation(variation, i)
            
            task = self.synthesize_segment(
                segment,
                persona,
                segment_variation
            )
            tasks.append(task)
        
        # Gather results
        segment_outputs = await asyncio.gather(*tasks)
        
        # Concatenate with smooth transitions
        final_audio = self.concatenate_audio_smoothly(segment_outputs)
        
        return VoiceOutput(
            audio_data=final_audio,
            duration=sum(s.duration for s in segment_outputs),
            format=segment_outputs[0].format,
            persona=persona,
            variation=variation
        )
```

## ðŸŽ¯ Usage Examples

### Example 1: Emotional Support Response

```python
# User shares difficult memory
transcript = "I just remembered the day my father passed away..."
analysis = TranscriptAnalysis(
    emotional_intensity=0.9,
    primary_emotion="grief",
    themes=["loss", "family", "memory"]
)

# Persona selection
selected = persona_selector.select_persona(
    agent_type="SpiritualAgent",
    transcript_analysis=analysis,
    theory_of_mind=user_theory,
    conversation_context=context
)

# Result: Friend persona with Comfort variation
assert selected.persona == VoicePersona.FRIEND
assert selected.variation == FriendVoiceVariations.COMFORT

# Voice synthesis
response_text = "I'm so sorry for your loss. That must be such a difficult memory..."
voice_output = await voice_client.synthesize_speech(
    text=response_text,
    persona=selected.persona,
    variation=selected.variation
)
```

### Example 2: Exciting Project Idea

```python
# User has exciting project idea
transcript = "I just figured out how to solve the authentication problem!"
analysis = TranscriptAnalysis(
    emotional_intensity=0.8,
    primary_emotion="excitement",
    themes=["innovation", "problem_solving", "breakthrough"]
)

# Persona selection
selected = persona_selector.select_persona(
    agent_type="GitHubAgent",
    transcript_analysis=analysis,
    theory_of_mind=user_theory,
    conversation_context=context
)

# Result: Builder persona with Excited Discovery variation
assert selected.persona == VoicePersona.BUILDER
assert selected.variation == BuilderVoiceVariations.EXCITED_DISCOVERY

# Voice synthesis with enthusiasm
response_text = "That's brilliant! Tell me more about your solution!"
voice_output = await voice_client.synthesize_speech(
    text=response_text,
    persona=selected.persona,
    variation=selected.variation
)
```

## ðŸ”® Future Enhancements

### 1. Adaptive Voice Learning
- Learn user's preferred voice characteristics
- Adjust personas based on user feedback
- Create custom voice blends

### 2. Emotional Mirroring
- Match user's emotional energy
- Provide appropriate emotional support
- Avoid jarring tonal mismatches

### 3. Contextual Voice Memory
- Remember voice preferences for topics
- Maintain consistency in long conversations
- Build voice-based rapport

### 4. Multi-Modal Expression
- Coordinate voice with visual elements
- Add appropriate sound effects
- Create immersive audio experiences

---

This persona voice architecture creates a deeply personalized, emotionally intelligent voice interface that adapts to the user's needs while maintaining consistent character for each agent.